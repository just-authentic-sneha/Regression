{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Regression"
      ],
      "metadata": {
        "id": "Z6q8svCa_Fdj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 1. What is Simple Linear Regression?\n",
        "Simple Linear Regression is a statistical method that models the relationship between a dependent variable and one independent variable using a straight line.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. What are the key assumptions of Simple Linear Regression?\n",
        "- Linearity\n",
        "- Independence of errors\n",
        "- Homoscedasticity (constant variance of errors)\n",
        "- Normal distribution of residuals\n",
        "\n",
        "---\n",
        "\n",
        "### 3. What does the coefficient m represent in the equation Y = mX + c?\n",
        "The coefficient **m** represents the **slope** of the line, indicating the change in Y for a one-unit change in X.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. What does the intercept c represent in the equation Y = mX + c?\n",
        "The intercept **c** represents the value of Y when X is 0.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. How do we calculate the slope m in Simple Linear Regression?\n",
        "The slope is calculated as:  \n",
        "**m = Σ[(Xi - X̄)(Yi - Ȳ)] / Σ[(Xi - X̄)²]**\n",
        "\n",
        "---\n",
        "\n",
        "### 6. What is the purpose of the least squares method in Simple Linear Regression?\n",
        "The least squares method minimizes the sum of the squared differences between observed and predicted values, providing the best-fitting line.\n",
        "\n",
        "---\n",
        "\n",
        "### 7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
        "R² indicates the proportion of the variance in the dependent variable explained by the independent variable. It ranges from 0 to 1.\n",
        "\n",
        "---\n",
        "\n",
        "### 8. What is Multiple Linear Regression?\n",
        "Multiple Linear Regression models the relationship between one dependent variable and two or more independent variables.\n",
        "\n",
        "---\n",
        "\n",
        "### 9. What is the main difference between Simple and Multiple Linear Regression?\n",
        "Simple Linear Regression uses one independent variable, while Multiple Linear Regression uses multiple independent variables.\n",
        "\n",
        "---\n",
        "\n",
        "### 10. What are the key assumptions of Multiple Linear Regression?\n",
        "- Linearity\n",
        "- Independence of errors\n",
        "- Homoscedasticity\n",
        "- No multicollinearity\n",
        "- Normal distribution of residuals\n",
        "\n",
        "---\n",
        "\n",
        "### 11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "Heteroscedasticity refers to unequal error variances. It can lead to inefficient estimates and unreliable hypothesis tests.\n",
        "\n",
        "---\n",
        "\n",
        "### 12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "- Remove or combine correlated predictors\n",
        "- Use dimensionality reduction (e.g., PCA)\n",
        "- Apply regularization techniques (e.g., Ridge or Lasso)\n",
        "\n",
        "---\n",
        "\n",
        "### 13. What are some common techniques for transforming categorical variables for use in regression models?\n",
        "- One-hot encoding\n",
        "- Label encoding\n",
        "- Dummy variables\n",
        "\n",
        "---\n",
        "\n",
        "### 14. What is the role of interaction terms in Multiple Linear Regression?\n",
        "Interaction terms model the combined effect of two or more variables, capturing relationships that aren't purely additive.\n",
        "\n",
        "---\n",
        "\n",
        "### 15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "In Simple Linear Regression, the intercept is the expected value of Y when X is zero. In Multiple Linear Regression, it's the expected value of Y when all independent variables are zero.\n",
        "\n",
        "---\n",
        "\n",
        "### 16. What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "The slope indicates how much the dependent variable is expected to change with a one-unit change in the predictor. It directly affects prediction outcomes.\n",
        "\n",
        "---\n",
        "\n",
        "### 17. How does the intercept in a regression model provide context for the relationship between variables?\n",
        "The intercept provides a baseline value for the dependent variable, offering insight into the starting point of the model when all predictors are zero.\n",
        "\n",
        "---\n",
        "\n",
        "### 18. What are the limitations of using R² as a sole measure of model performance?\n",
        "- It doesn't indicate whether the model is biased\n",
        "- Can be artificially inflated by adding irrelevant variables\n",
        "- Doesn't measure predictive accuracy\n",
        "\n",
        "---\n",
        "\n",
        "### 19. How would you interpret a large standard error for a regression coefficient?\n",
        "A large standard error suggests that the coefficient estimate is unstable and may not be significantly different from zero.\n",
        "\n",
        "---\n",
        "\n",
        "### 20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "Heteroscedasticity appears as a funnel shape in residual plots. It violates regression assumptions and leads to inefficient estimates.\n",
        "\n",
        "---\n",
        "\n",
        "### 21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
        "It may indicate that the model includes unnecessary predictors that do not contribute to explaining the variance in the dependent variable.\n",
        "\n",
        "---\n",
        "\n",
        "### 22. Why is it important to scale variables in Multiple Linear Regression?\n",
        "Scaling ensures that variables contribute equally to the model, especially important for algorithms sensitive to magnitude and when using regularization.\n",
        "\n",
        "---\n",
        "\n",
        "### 23. What is polynomial regression?\n",
        "Polynomial regression models the relationship between the dependent variable and the independent variable(s) as an nth-degree polynomial.\n",
        "\n",
        "---\n",
        "\n",
        "### 24. How does polynomial regression differ from linear regression?\n",
        "Linear regression fits a straight line, while polynomial regression fits a curved line by including higher-degree terms of the independent variable.\n",
        "\n",
        "---\n",
        "\n",
        "### 25. When is polynomial regression used?\n",
        "It is used when the data shows a non-linear relationship that cannot be captured by a straight line.\n",
        "\n",
        "---\n",
        "\n",
        "### 26. What is the general equation for polynomial regression?\n",
        "**Y = β₀ + β₁X + β₂X² + β₃X³ + ... + βnXⁿ + ε**\n",
        "\n",
        "---\n",
        "\n",
        "### 27. Can polynomial regression be applied to multiple variables?\n",
        "Yes, it can be extended to multiple predictors by including polynomial terms for each variable and their interactions.\n",
        "\n",
        "---\n",
        "\n",
        "### 28. What are the limitations of polynomial regression?\n",
        "- Prone to overfitting with high-degree polynomials\n",
        "- Difficult to interpret\n",
        "- Sensitive to outliers\n",
        "\n",
        "---\n",
        "\n",
        "### 29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "- Cross-validation\n",
        "- Adjusted R²\n",
        "- AIC/BIC\n",
        "- Residual analysis\n",
        "\n",
        "---\n",
        "\n",
        "### 30. Why is visualization important in polynomial regression?\n",
        "Visualization helps in understanding the data fit, identifying overfitting, and interpreting the relationship between variables.\n",
        "\n",
        "---\n",
        "\n",
        "### 31. How is polynomial regression implemented in Python?\n",
        "```python\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Example: 3rd-degree polynomial\n",
        "model = make_pipeline(PolynomialFeatures(degree=3), LinearRegression())\n",
        "model.fit(X, y)\n",
        "y_pred = model.predict(X)\n"
      ],
      "metadata": {
        "id": "9PK-T4Za_IFy"
      }
    }
  ]
}